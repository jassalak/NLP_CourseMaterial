{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: The purpose of this homework will be to examine the similarity of a number of articles contained in the data directory. Specifically, the intent is to implement different document similarity techniques and then see how similar these documents are using said techniques. Finally, we would like to present the similarity in a table or heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "In order to begin this exercise, we will first need to iterate over the data directory and read the content of each file. We will store the data from each file in a hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_homework_5_data_directory = \"PUT YOUR FULL PATH HERE\"\n",
    "\n",
    "article_hash = {} # this hash should serve to represent the content of the files in the data directory\n",
    "# use the filename as the hash key and the value will be the text of the file\n",
    "# thus you would be able to retrieve an individual documents text like: article_hash[\"article_1\"]\n",
    "\n",
    "# here we will get a list of the filenames of things contained in the data directory\n",
    "files = [f for f in listdir(path_to_homework_5_data_directory) if isfile(join(path_to_homework_5_data_directory, f))]\n",
    "\n",
    "# here you will iterate over all the files contained in the directory\n",
    "for loop... # implement the rest of this line and the content of the loop\n",
    "    # you will read the content of the file and put the text into the hash\n",
    "    # article_hash[filename_variable] = the content of the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "Text processing. Now that you have the content of the files read into a hash you will be able to process them. Specifically, you should perhaps employ sentence segmentation, tokenization, and stemming to get a new representation of the document. Here you will want to build a sufficiently flexible approach so that you can try out several different pre-processing strategies to see how it affects your similarity scores.\n",
    "\n",
    "We'll create a new hash that contains the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup a new hash to store the results in\n",
    "processed_article_hash = {}\n",
    "\n",
    "# iterate through the keys, i.e. document ids, in the hash to pull out the stored text and process\n",
    "for key in article_hash.keys():\n",
    "    text_of_article = article_hash[key]\n",
    "    \n",
    "    # here you can apply your segmentation, tokenization, and stemming steps as you see fit\n",
    "    \n",
    "    processed_article_hash[key] = # the result of whatever steps you use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "Implement two similarity techniques. \n",
    "We would like to examine the Jacard Similarity and Cosine.\n",
    "\n",
    "Jacardian Similarity: here we want to identify the set of words in two documents that overlap and then divide that by the count of unique words across both documents.\n",
    "\n",
    "Cosine Similarity: Here we want to create vector representations for each document. Specifically, we want to come up with a vector that is based on the list of all words that occur across both documents. Then for each document we will create a vector that includes the counts of the number of time a word occurs in the document.\n",
    "\n",
    "So if the document 1 is: \"the ship sails at midnight\" and document 2 is: \"the crow flies at noon.\" We would be creating a vector like: [the, ship, sails, at, midnight, crow, flies, noon]. Then we would calculate the values of the vector for each document. For document 1: [1,1,1,1,1,0,0,0] and for document 2: [1,0,0,1,0,1,1,1]. With these two vectors we would simply take the dot product and that would provide the cosine similarity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jacardian_distance(document_1_data, document_2_data):\n",
    "    words_in_doc_1_not_in_doc_2 = \n",
    "    words_in_doc_2_not_in_doc_1 = \n",
    "    words_in_both_doc_1_and_doc_2 = \n",
    "    \n",
    "    jacardian = # divide the counts appropiately\n",
    "    \n",
    "    return jacardian\n",
    "    \n",
    "def cosine_similarity(document_1_data, document_2_data):\n",
    "    document_vector_word_index = [] # here fill this with an ordered list of all the unique words across both documents\n",
    "    document_1_vector = [] # fill in the array with the frequency of the words in the document\n",
    "    document_2_vector = [] # fill in the array with the frequency of the words in the document\n",
    "    \n",
    "    return dot_product_of_two_document_vectors # you can refer to the numpy information on how to calculate the dot product of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "Now that we have our two similarity measures, we want to examine each document relative to each other and calculate their similarity. \n",
    "\n",
    "So we will want to create two tables that show the document similarities using both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a variable to store your table data... you could use a hash or some other data structure. \n",
    "# We just want it to identify which document is being compared to which other document.\n",
    "\n",
    "data_structure_for_jacard_similarity = #\n",
    "data_structure_for_cosine_similarity = #\n",
    "\n",
    "for doc_1_key in article_hash.keys():\n",
    "    for doc_2_key in article_hash.keys():\n",
    "        # we have the nested for loops as one way to compare each document to each other document\n",
    "        data_structure_for_jacard_similarity[doc_1_key][doc_2_key] = jacardian_distance(doc_1_processed_text, doc_2_processed_text)\n",
    "        data_structure_for_cosine_similarity[doc_1_key][doc_2_key] = jacardian_distance(doc_1_processed_text, doc_2_processed_text)\n",
    "\n",
    "# finally, find some way to present this data back. Either as a straight table or a heatmap.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:\n",
    "You should now have two different similarity mechanisms. What do your results suggest? From perusing the documents, do you think the suggested ones are similar or not? Does tokenization, stemming, stop word removal or anything else improve your results?\n",
    "\n",
    "Write a brief description of your reactions to identifying these similar documents and what measures and pre-processing steps you think worked best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- put your comments here -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
